{
 "metadata": {
  "name": "",
  "signature": "sha256:8322326ce20ef64c4d2928313de39ba64d8e8e17ee9496dd999ca162f59808e5"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import print_function\n",
      "import numpy as np\n",
      "import tensorflow as tf\n",
      "import math as math\n",
      "import argparse\n",
      "\n",
      "argsdataset = \"inception_3a.csv\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "\n",
      "def file_len(fname):\n",
      "    with open(fname) as f:\n",
      "        for i, l in enumerate(f):\n",
      "            pass\n",
      "    return i + 1\n",
      "\n",
      "def read_from_csv(filename_queue):\n",
      "  reader = tf.TextLineReader(skip_header_lines=1)\n",
      "  _, csv_row = reader.read(filename_queue)\n",
      "  record_defaults = [[\"jpg\"],[\"1\"]]\n",
      "  colHour,colQuarter = tf.decode_csv(csv_row, record_defaults=record_defaults)\n",
      "  features = tf.pack([colHour,colQuarter])  \n",
      "  return features\n",
      "\n",
      "def input_pipeline(batch_size, num_epochs=None):\n",
      "  filename_queue = tf.train.string_input_producer([argsdataset], num_epochs=num_epochs, shuffle=True)  \n",
      "  example = read_from_csv(filename_queue)\n",
      "  min_after_dequeue = 10000\n",
      "  capacity = min_after_dequeue + 3 * batch_size\n",
      "  example_batch = tf.train.shuffle_batch(\n",
      "      [example], batch_size=batch_size, capacity=capacity,\n",
      "      min_after_dequeue=min_after_dequeue)\n",
      "  return example_batch\n",
      "\n",
      "file_length = file_len(argsdataset) - 1\n",
      "#examples = input_pipeline(file_length, 1)\n",
      "filename_queue = tf.train.string_input_producer(tf.constant([argsdataset]));\n",
      "reader = tf.TextLineReader()\n",
      "key, value = reader.read(filename_queue)\n",
      "record_defaults = [[0]] * 2\n",
      "outputs = tf.decode_csv(value, record_defaults=record_defaults)\n",
      "\n",
      "\n",
      "\n",
      "with tf.Session() as sess:\n",
      "  tf.initialize_all_variables().run()\n",
      "\n",
      "  # start populating filename queue\n",
      "  coord = tf.train.Coordinator()\n",
      "  threads = tf.train.start_queue_runners(coord=coord)\n",
      "\n",
      "  try:\n",
      "    while not coord.should_stop():\n",
      "      example_batch  = sess.run([examples])\n",
      "      print(example_batch)\n",
      "  except tf.errors.OutOfRangeError:\n",
      "    print('Done training, epoch reached')\n",
      "  finally:\n",
      "    coord.request_stop()\n",
      "  print(\"done\")\n",
      "  coord.join(threads) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "WARNING:tensorflow:From <ipython-input-25-b407eb950d6a>:44 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
        "Instructions for updating:\n",
        "Use `tf.global_variables_initializer` instead.\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:tensorflow:From <ipython-input-25-b407eb950d6a>:44 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
        "Instructions for updating:\n",
        "Use `tf.global_variables_initializer` instead.\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.FailedPreconditionError'>, Attempting to use uninitialized value input_producer_5/limit_epochs/epochs\n",
        "\t [[Node: input_producer_5/limit_epochs/CountUpTo = CountUpTo[T=DT_INT64, _class=[\"loc:@input_producer_5/limit_epochs/epochs\"], limit=1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input_producer_5/limit_epochs/epochs)]]\n",
        "\n",
        "Caused by op u'input_producer_5/limit_epochs/CountUpTo', defined at:\n",
        "  File \"<string>\", line 1, in <module>\n",
        "  File \"/usr/lib/python2.7/dist-packages/IPython/kernel/zmq/kernelapp.py\", line 469, in main\n",
        "    app.start()\n",
        "  File \"/usr/lib/python2.7/dist-packages/IPython/kernel/zmq/kernelapp.py\", line 459, in start\n",
        "    ioloop.IOLoop.instance().start()\n",
        "  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py\", line 162, in start\n",
        "    super(ZMQIOLoop, self).start()\n",
        "  File \"/usr/lib/python2.7/dist-packages/tornado/ioloop.py\", line 866, in start\n",
        "    handler_func(fd_obj, events)\n",
        "  File \"/usr/lib/python2.7/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n",
        "    return fn(*args, **kwargs)\n",
        "  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n",
        "    self._handle_recv()\n",
        "  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n",
        "    self._run_callback(callback, msg)\n",
        "  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n",
        "    callback(*args, **kwargs)\n",
        "  File \"/usr/lib/python2.7/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n",
        "    return fn(*args, **kwargs)\n",
        "  File \"/usr/lib/python2.7/dist-packages/IPython/kernel/zmq/ipkernel.py\", line 281, in dispatcher\n",
        "    return self.dispatch_shell(stream, msg)\n",
        "  File \"/usr/lib/python2.7/dist-packages/IPython/kernel/zmq/ipkernel.py\", line 245, in dispatch_shell\n",
        "    handler(stream, idents, msg)\n",
        "  File \"/usr/lib/python2.7/dist-packages/IPython/kernel/zmq/ipkernel.py\", line 389, in execute_request\n",
        "    shell.run_cell(code, store_history=store_history, silent=silent)\n",
        "  File \"/usr/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2741, in run_cell\n",
        "    interactivity=interactivity, compiler=compiler)\n",
        "  File \"/usr/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2827, in run_ast_nodes\n",
        "    if self.run_code(code):\n",
        "  File \"/usr/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2883, in run_code\n",
        "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
        "  File \"<ipython-input-16-7ad115997ab7>\", line 35, in <module>\n",
        "    examples, labels = input_pipeline(file_length, 1)\n",
        "  File \"<ipython-input-16-7ad115997ab7>\", line 25, in input_pipeline\n",
        "    filename_queue = tf.train.string_input_producer([argsdataset], num_epochs=num_epochs, shuffle=True)\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py\", line 230, in string_input_producer\n",
        "    cancel_op=cancel_op)\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py\", line 156, in input_producer\n",
        "    input_tensor = limit_epochs(input_tensor, num_epochs)\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py\", line 96, in limit_epochs\n",
        "    counter = epochs.count_up_to(num_epochs)\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py\", line 652, in count_up_to\n",
        "    return state_ops.count_up_to(self._variable, limit=limit)\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_state_ops.py\", line 126, in count_up_to\n",
        "    result = _op_def_lib.apply_op(\"CountUpTo\", ref=ref, limit=limit, name=name)\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\n",
        "    op_def=op_def)\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\n",
        "    original_op=self._default_original_op, op_def=op_def)\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\n",
        "    self._traceback = _extract_stack()\n",
        "\n",
        "FailedPreconditionError (see above for traceback): Attempting to use uninitialized value input_producer_5/limit_epochs/epochs\n",
        "\t [[Node: input_producer_5/limit_epochs/CountUpTo = CountUpTo[T=DT_INT64, _class=[\"loc:@input_producer_5/limit_epochs/epochs\"], limit=1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input_producer_5/limit_epochs/epochs)]]\n",
        "\n",
        "Done training, epoch reached"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "done\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.FailedPreconditionError'>, Attempting to use uninitialized value input_producer_5/limit_epochs/epochs\n",
        "\t [[Node: input_producer_5/limit_epochs/CountUpTo = CountUpTo[T=DT_INT64, _class=[\"loc:@input_producer_5/limit_epochs/epochs\"], limit=1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input_producer_5/limit_epochs/epochs)]]\n",
        "\n",
        "Caused by op u'input_producer_5/limit_epochs/CountUpTo', defined at:\n",
        "  File \"<string>\", line 1, in <module>\n",
        "  File \"/usr/lib/python2.7/dist-packages/IPython/kernel/zmq/kernelapp.py\", line 469, in main\n",
        "    app.start()\n",
        "  File \"/usr/lib/python2.7/dist-packages/IPython/kernel/zmq/kernelapp.py\", line 459, in start\n",
        "    ioloop.IOLoop.instance().start()\n",
        "  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py\", line 162, in start\n",
        "    super(ZMQIOLoop, self).start()\n",
        "  File \"/usr/lib/python2.7/dist-packages/tornado/ioloop.py\", line 866, in start\n",
        "    handler_func(fd_obj, events)\n",
        "  File \"/usr/lib/python2.7/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n",
        "    return fn(*args, **kwargs)\n",
        "  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n",
        "    self._handle_recv()\n",
        "  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n",
        "    self._run_callback(callback, msg)\n",
        "  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n",
        "    callback(*args, **kwargs)\n",
        "  File \"/usr/lib/python2.7/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n",
        "    return fn(*args, **kwargs)\n",
        "  File \"/usr/lib/python2.7/dist-packages/IPython/kernel/zmq/ipkernel.py\", line 281, in dispatcher\n",
        "    return self.dispatch_shell(stream, msg)\n",
        "  File \"/usr/lib/python2.7/dist-packages/IPython/kernel/zmq/ipkernel.py\", line 245, in dispatch_shell\n",
        "    handler(stream, idents, msg)\n",
        "  File \"/usr/lib/python2.7/dist-packages/IPython/kernel/zmq/ipkernel.py\", line 389, in execute_request\n",
        "    shell.run_cell(code, store_history=store_history, silent=silent)\n",
        "  File \"/usr/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2741, in run_cell\n",
        "    interactivity=interactivity, compiler=compiler)\n",
        "  File \"/usr/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2827, in run_ast_nodes\n",
        "    if self.run_code(code):\n",
        "  File \"/usr/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2883, in run_code\n",
        "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
        "  File \"<ipython-input-16-7ad115997ab7>\", line 35, in <module>\n",
        "    examples, labels = input_pipeline(file_length, 1)\n",
        "  File \"<ipython-input-16-7ad115997ab7>\", line 25, in input_pipeline\n",
        "    filename_queue = tf.train.string_input_producer([argsdataset], num_epochs=num_epochs, shuffle=True)\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py\", line 230, in string_input_producer\n",
        "    cancel_op=cancel_op)\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py\", line 156, in input_producer\n",
        "    input_tensor = limit_epochs(input_tensor, num_epochs)\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py\", line 96, in limit_epochs\n",
        "    counter = epochs.count_up_to(num_epochs)\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py\", line 652, in count_up_to\n",
        "    return state_ops.count_up_to(self._variable, limit=limit)\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_state_ops.py\", line 126, in count_up_to\n",
        "    result = _op_def_lib.apply_op(\"CountUpTo\", ref=ref, limit=limit, name=name)\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\n",
        "    op_def=op_def)\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\n",
        "    original_op=self._default_original_op, op_def=op_def)\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\n",
        "    self._traceback = _extract_stack()\n",
        "\n",
        "FailedPreconditionError (see above for traceback): Attempting to use uninitialized value input_producer_5/limit_epochs/epochs\n",
        "\t [[Node: input_producer_5/limit_epochs/CountUpTo = CountUpTo[T=DT_INT64, _class=[\"loc:@input_producer_5/limit_epochs/epochs\"], limit=1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input_producer_5/limit_epochs/epochs)]]\n",
        "\n"
       ]
      },
      {
       "ename": "FailedPreconditionError",
       "evalue": "Attempting to use uninitialized value input_producer_5/limit_epochs/epochs\n\t [[Node: input_producer_5/limit_epochs/CountUpTo = CountUpTo[T=DT_INT64, _class=[\"loc:@input_producer_5/limit_epochs/epochs\"], limit=1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input_producer_5/limit_epochs/epochs)]]\n\nCaused by op u'input_producer_5/limit_epochs/CountUpTo', defined at:\n  File \"<string>\", line 1, in <module>\n  File \"/usr/lib/python2.7/dist-packages/IPython/kernel/zmq/kernelapp.py\", line 469, in main\n    app.start()\n  File \"/usr/lib/python2.7/dist-packages/IPython/kernel/zmq/kernelapp.py\", line 459, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/lib/python2.7/dist-packages/tornado/ioloop.py\", line 866, in start\n    handler_func(fd_obj, events)\n  File \"/usr/lib/python2.7/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/lib/python2.7/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/lib/python2.7/dist-packages/IPython/kernel/zmq/ipkernel.py\", line 281, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/lib/python2.7/dist-packages/IPython/kernel/zmq/ipkernel.py\", line 245, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/lib/python2.7/dist-packages/IPython/kernel/zmq/ipkernel.py\", line 389, in execute_request\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2741, in run_cell\n    interactivity=interactivity, compiler=compiler)\n  File \"/usr/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2827, in run_ast_nodes\n    if self.run_code(code):\n  File \"/usr/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2883, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-16-7ad115997ab7>\", line 35, in <module>\n    examples, labels = input_pipeline(file_length, 1)\n  File \"<ipython-input-16-7ad115997ab7>\", line 25, in input_pipeline\n    filename_queue = tf.train.string_input_producer([argsdataset], num_epochs=num_epochs, shuffle=True)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py\", line 230, in string_input_producer\n    cancel_op=cancel_op)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py\", line 156, in input_producer\n    input_tensor = limit_epochs(input_tensor, num_epochs)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py\", line 96, in limit_epochs\n    counter = epochs.count_up_to(num_epochs)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py\", line 652, in count_up_to\n    return state_ops.count_up_to(self._variable, limit=limit)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_state_ops.py\", line 126, in count_up_to\n    result = _op_def_lib.apply_op(\"CountUpTo\", ref=ref, limit=limit, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\n    self._traceback = _extract_stack()\n\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value input_producer_5/limit_epochs/epochs\n\t [[Node: input_producer_5/limit_epochs/CountUpTo = CountUpTo[T=DT_INT64, _class=[\"loc:@input_producer_5/limit_epochs/epochs\"], limit=1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input_producer_5/limit_epochs/epochs)]]\n",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-25-b407eb950d6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mcoord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m   \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"done\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m   \u001b[0mcoord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthreads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.pyc\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, threads, stop_grace_period_secs)\u001b[0m\n\u001b[1;32m    384\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_registered_threads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exc_info_to_raise\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m         \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exc_info_to_raise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0mstragglers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m         raise RuntimeError(\n",
        "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/queue_runner_impl.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, sess, enqueue_op, coord)\u001b[0m\n\u001b[1;32m    232\u001b[0m           \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m           \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menqueue_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_queue_closed_exception_types\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=catching-non-exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m           \u001b[0;31m# This exception indicates that a queue was closed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 964\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    965\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1014\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1015\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
        "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value input_producer_5/limit_epochs/epochs\n\t [[Node: input_producer_5/limit_epochs/CountUpTo = CountUpTo[T=DT_INT64, _class=[\"loc:@input_producer_5/limit_epochs/epochs\"], limit=1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input_producer_5/limit_epochs/epochs)]]\n\nCaused by op u'input_producer_5/limit_epochs/CountUpTo', defined at:\n  File \"<string>\", line 1, in <module>\n  File \"/usr/lib/python2.7/dist-packages/IPython/kernel/zmq/kernelapp.py\", line 469, in main\n    app.start()\n  File \"/usr/lib/python2.7/dist-packages/IPython/kernel/zmq/kernelapp.py\", line 459, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/lib/python2.7/dist-packages/tornado/ioloop.py\", line 866, in start\n    handler_func(fd_obj, events)\n  File \"/usr/lib/python2.7/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/lib/python2.7/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/lib/python2.7/dist-packages/IPython/kernel/zmq/ipkernel.py\", line 281, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/lib/python2.7/dist-packages/IPython/kernel/zmq/ipkernel.py\", line 245, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/lib/python2.7/dist-packages/IPython/kernel/zmq/ipkernel.py\", line 389, in execute_request\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2741, in run_cell\n    interactivity=interactivity, compiler=compiler)\n  File \"/usr/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2827, in run_ast_nodes\n    if self.run_code(code):\n  File \"/usr/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2883, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-16-7ad115997ab7>\", line 35, in <module>\n    examples, labels = input_pipeline(file_length, 1)\n  File \"<ipython-input-16-7ad115997ab7>\", line 25, in input_pipeline\n    filename_queue = tf.train.string_input_producer([argsdataset], num_epochs=num_epochs, shuffle=True)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py\", line 230, in string_input_producer\n    cancel_op=cancel_op)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py\", line 156, in input_producer\n    input_tensor = limit_epochs(input_tensor, num_epochs)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py\", line 96, in limit_epochs\n    counter = epochs.count_up_to(num_epochs)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py\", line 652, in count_up_to\n    return state_ops.count_up_to(self._variable, limit=limit)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_state_ops.py\", line 126, in count_up_to\n    result = _op_def_lib.apply_op(\"CountUpTo\", ref=ref, limit=limit, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\n    self._traceback = _extract_stack()\n\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value input_producer_5/limit_epochs/epochs\n\t [[Node: input_producer_5/limit_epochs/CountUpTo = CountUpTo[T=DT_INT64, _class=[\"loc:@input_producer_5/limit_epochs/epochs\"], limit=1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input_producer_5/limit_epochs/epochs)]]\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import csv\n",
      "f = open(argsdataset);\n",
      "csv.field_size_limit(4521984*2);\n",
      "\n",
      "def file_len(fname):\n",
      "    with open(fname) as f:\n",
      "        for i, l in enumerate(f):\n",
      "            pass\n",
      "    return i + 1\n",
      "\n",
      "\n",
      "flen = file_len(argsdataset);\n",
      "print(\"File size =\", flen);  \n",
      "\n",
      "reader = csv.reader(f)\n",
      "def readfile(reader,batch_size = 1):\n",
      "    b = 0;\n",
      "    read_features  = [];\n",
      "    for row in reader:\n",
      "        features = [x for x in row[1:]][0];\n",
      "        features = features.replace('[','');\n",
      "        features = features.replace(']','');\n",
      "        featureslist = features.split();\n",
      "        features = [float(x) for x in (featureslist) ];\n",
      "        if(len(read_features) == 0):\n",
      "            read_features.append(features);\n",
      "        else:\n",
      "            read_features = np.vstack((read_features,features));\n",
      "        if(b == batch_size-1):\n",
      "            break\n",
      "        b = b+1;\n",
      "    return read_features;\n",
      " \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "File size = 185\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Parameters\n",
      "learning_rate = 0.01\n",
      "training_epochs = 20\n",
      "batch_size = 1\n",
      "display_step = 1\n",
      "\n",
      "# Network Parameters\n",
      "n_hidden_1 = 512 # 1st layer num features\n",
      "n_hidden_2 = 128 # 2nd layer num features\n",
      "n_input = 256*256 #  \n",
      "\n",
      "# tf Graph input (only pictures)\n",
      "X = tf.placeholder(\"float\", [None, n_input])\n",
      "\n",
      "weights = {\n",
      "    'encoder_h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
      "    'encoder_h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
      "    'decoder_h1': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_1])),\n",
      "    'decoder_h2': tf.Variable(tf.random_normal([n_hidden_1, n_input])),\n",
      "}\n",
      "biases = {\n",
      "    'encoder_b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
      "    'encoder_b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
      "    'decoder_b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
      "    'decoder_b2': tf.Variable(tf.random_normal([n_input])),\n",
      "}\n",
      "\n",
      "\n",
      "# Building the encoder\n",
      "def encoder(x):\n",
      "    # Encoder Hidden layer with sigmoid activation #1\n",
      "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['encoder_h1']),\n",
      "                                   biases['encoder_b1']))\n",
      "    # Decoder Hidden layer with sigmoid activation #2\n",
      "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['encoder_h2']),\n",
      "                                   biases['encoder_b2']))\n",
      "    return layer_2\n",
      "\n",
      "\n",
      "# Building the decoder\n",
      "def decoder(x):\n",
      "    # Encoder Hidden layer with sigmoid activation #1\n",
      "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['decoder_h1']),\n",
      "                                   biases['decoder_b1']))\n",
      "    # Decoder Hidden layer with sigmoid activation #2\n",
      "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['decoder_h2']),\n",
      "                                   biases['decoder_b2']))\n",
      "    return layer_2\n",
      "\n",
      "# Construct model\n",
      "encoder_op = encoder(X)\n",
      "decoder_op = decoder(encoder_op)\n",
      "\n",
      "# Prediction\n",
      "y_pred = decoder_op\n",
      "# Targets (Labels) are the input data.\n",
      "y_true = X\n",
      "\n",
      "# Define loss and optimizer, minimize the squared error\n",
      "cost = tf.reduce_mean(tf.pow(y_true - y_pred, 2))\n",
      "optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(cost)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Initializing the variables\n",
      "init = tf.initialize_all_variables()\n",
      "batch = 1;\n",
      "# Launch the graph\n",
      "with tf.Session() as sess:\n",
      "    sess.run(init)\n",
      "    total_batch = int(flen/batch_size)\n",
      "    # Training cycle\n",
      "    for epoch in range(training_epochs):\n",
      "        # Loop over all batches\n",
      "        for i in range(total_batch):\n",
      "            batch_xs = readfile(reader,batch_size)\n",
      "            # Run optimization op (backprop) and cost op (to get loss value)\n",
      "            _, c = sess.run([optimizer, cost], feed_dict={X: np.array(batch_xs)})\n",
      "        # Display logs per epoch step\n",
      "        if epoch % display_step == 0:\n",
      "            print(\"Epoch:\", '%04d' % (epoch+1),\n",
      "                  \"cost=\", \"{:.9f}\".format(c))\n",
      "\n",
      "    print(\"Optimization Finished!\")\n",
      "\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "WARNING:tensorflow:From <ipython-input-8-618bb5c9bba5>:2 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
        "Instructions for updating:\n",
        "Use `tf.global_variables_initializer` instead.\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:tensorflow:From <ipython-input-8-618bb5c9bba5>:2 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
        "Instructions for updating:\n",
        "Use `tf.global_variables_initializer` instead.\n"
       ]
      },
      {
       "ename": "ValueError",
       "evalue": "Cannot feed value of shape (0,) for Tensor u'Placeholder:0', which has shape '(?, 65536)'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-8-618bb5c9bba5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mbatch_xs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreadfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;31m# Run optimization op (backprop) and cost op (to get loss value)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_xs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Display logs per epoch step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdisplay_step\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    941\u001b[0m                 \u001b[0;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m                 \u001b[0;34m'which has shape %r'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m                 % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m    944\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (0,) for Tensor u'Placeholder:0', which has shape '(?, 65536)'"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\" Deep Auto-Encoder implementation\n",
      "\t\n",
      "\tAn auto-encoder works as follows:\n",
      "\n",
      "\tData of dimension k is reduced to a lower dimension j using a matrix multiplication:\n",
      "\tsoftmax(W*x + b)  = x'\n",
      "\t\n",
      "\twhere W is matrix from R^k --> R^j\n",
      "\n",
      "\tA reconstruction matrix W' maps back from R^j --> R^k\n",
      "\n",
      "\tso our reconstruction function is softmax'(W' * x' + b') \n",
      "\n",
      "\tNow the point of the auto-encoder is to create a reduction matrix (values for W, b) \n",
      "\tthat is \"good\" at reconstructing  the original data. \n",
      "\n",
      "\tThus we want to minimize  ||softmax'(W' * (softmax(W *x+ b)) + b')  - x||\n",
      "\n",
      "\tA deep auto-encoder is nothing more than stacking successive layers of these reductions.\n",
      "\"\"\"\n",
      "import tensorflow as tf\n",
      "import numpy as np\n",
      "import math\n",
      "import random\n",
      "\n",
      "def create(x, layer_sizes):\n",
      "\n",
      "\t# Build the encoding layers\n",
      "\tnext_layer_input = x\n",
      "\n",
      "\tencoding_matrices = []\n",
      "\tfor dim in layer_sizes:\n",
      "\t\tinput_dim = int(next_layer_input.get_shape()[1])\n",
      "\n",
      "\t\t# Initialize W using random values in interval [-1/sqrt(n) , 1/sqrt(n)]\n",
      "\t\tW = tf.Variable(tf.random_uniform([input_dim, dim], -1.0 / math.sqrt(input_dim), 1.0 / math.sqrt(input_dim)))\n",
      "\n",
      "\t\t# Initialize b to zero\n",
      "\t\tb = tf.Variable(tf.zeros([dim]))\n",
      "\n",
      "\t\t# We are going to use tied-weights so store the W matrix for later reference.\n",
      "\t\tencoding_matrices.append(W)\n",
      "\n",
      "\t\toutput = tf.nn.tanh(tf.matmul(next_layer_input,W) + b)\n",
      "\n",
      "\t\t# the input into the next layer is the output of this layer\n",
      "\t\tnext_layer_input = output\n",
      "\n",
      "\t# The fully encoded x value is now stored in the next_layer_input\n",
      "\tencoded_x = next_layer_input\n",
      "\n",
      "\t# build the reconstruction layers by reversing the reductions\n",
      "\tlayer_sizes.reverse()\n",
      "\tencoding_matrices.reverse()\n",
      "\n",
      "\n",
      "\tfor i, dim in enumerate(layer_sizes[1:] + [ int(x.get_shape()[1])]) :\n",
      "\t\t# we are using tied weights, so just lookup the encoding matrix for this step and transpose it\n",
      "\t\tW = tf.transpose(encoding_matrices[i])\n",
      "\t\tb = tf.Variable(tf.zeros([dim]))\n",
      "\t\toutput = tf.nn.tanh(tf.matmul(next_layer_input,W) + b)\n",
      "\t\tnext_layer_input = output\n",
      "\n",
      "\t# the fully encoded and reconstructed value of x is here:\n",
      "\treconstructed_x = next_layer_input\n",
      "\n",
      "\treturn {\n",
      "\t\t'encoded': encoded_x,\n",
      "\t\t'decoded': reconstructed_x,\n",
      "\t\t'cost' : tf.sqrt(tf.reduce_mean(tf.square(x-reconstructed_x)))\n",
      "\t}\n",
      "\n",
      "def simple_test():\n",
      "\tsess = tf.Session()\n",
      "\tx = tf.placeholder(\"float\", [None, 4])\n",
      "\tautoencoder = create(x, [2])\n",
      "\tinit = tf.initialize_all_variables()\n",
      "\tsess.run(init)\n",
      "\ttrain_step = tf.train.GradientDescentOptimizer(0.01).minimize(autoencoder['cost'])\n",
      "\n",
      "\n",
      "\t# Our dataset consists of two centers with gaussian noise w/ sigma = 0.1\n",
      "\tc1 = np.array([0,0,0.5,0])\n",
      "\tc2 = np.array([0.5,0,0,0])\n",
      "\n",
      "\t# do 1000 training steps\n",
      "\tfor i in range(2000):\n",
      "\t\t# make a batch of 100:\n",
      "\t\tbatch = []\n",
      "\t\tfor j in range(100):\n",
      "\t\t\t# pick a random centroid\n",
      "\t\t\tif (random.random() > 0.5):\n",
      "\t\t\t\tvec = c1\n",
      "\t\t\telse:\n",
      "\t\t\t\tvec = c2\n",
      "  \t\t\tbatch.append(np.random.normal(vec, 0.1))\n",
      "\t\tsess.run(train_step, feed_dict={x: np.array(batch)})\n",
      "\t\tif i % 100 == 0:\n",
      "\t\t\tprint (i, \" cost\", sess.run(autoencoder['cost'], feed_dict={x: batch}))\n",
      "\n",
      "\n",
      "def deep_test():\n",
      "\tsess = tf.Session()\n",
      "\tstart_dim = 5\n",
      "\tx = tf.placeholder(\"float\", [None, start_dim])\n",
      "\tautoencoder = create(x, [4, 3, 2])\n",
      "\tinit = tf.initialize_all_variables()\n",
      "\tsess.run(init)\n",
      "\ttrain_step = tf.train.GradientDescentOptimizer(0.5).minimize(autoencoder['cost'])\n",
      "\n",
      "\n",
      "\t# Our dataset consists of two centers with gaussian noise w/ sigma = 0.1\n",
      "\tc1 = np.zeros(start_dim)\n",
      "\tc1[0] = 1\n",
      "\n",
      "\tprint(c1)\n",
      "\n",
      "\tc2 = np.zeros(start_dim)\n",
      "\tc2[1] = 1\n",
      "\n",
      "\t# do 1000 training steps\n",
      "\tfor i in range(5000):\n",
      "\t\t# make a batch of 100:\n",
      "\t\tbatch = []\n",
      "\t\tfor j in range(1):\n",
      "\t\t\t# pick a random centroid\n",
      "\t\t\tif (random.random() > 0.5):\n",
      "\t\t\t\tvec = c1\n",
      "\t\t\telse:\n",
      "\t\t\t\tvec = c2\n",
      "  \t\t\tbatch.append(np.random.normal(vec, 0.1))\n",
      "\t\tsess.run(train_step, feed_dict={x: np.array(batch)})\n",
      "\t\tif i % 100 == 0:\n",
      "\t\t\tprint (i, \" cost\", sess.run(autoencoder['cost'], feed_dict={x: batch}))\n",
      "\t\t\tprint (i, \" original\", batch[0])\n",
      "\t\t\tprint (i, \" decoded\", sess.run(autoencoder['decoded'], feed_dict={x: batch}));\n",
      "\t\t\t\n",
      "if __name__ == '__main__':\n",
      "\tdeep_test()\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "WARNING:tensorflow:From <ipython-input-11-97304abe7c5f>:107 in deep_test.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
        "Instructions for updating:\n",
        "Use `tf.global_variables_initializer` instead.\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:tensorflow:From <ipython-input-11-97304abe7c5f>:107 in deep_test.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
        "Instructions for updating:\n",
        "Use `tf.global_variables_initializer` instead.\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 1.  0.  0.  0.  0.]\n",
        "0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  cost 0.241109\n",
        "0  original [-0.07991395  1.07976865  0.08738971  0.16118949 -0.08638327]\n",
        "0  decoded [[-0.1117205   0.55784458  0.12264266  0.1200109   0.03325737]]\n",
        "100"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  cost 0.154599\n",
        "100  original [ 1.023948   -0.09795687 -0.12957089  0.20632117  0.07845363]\n",
        "100  decoded [[ 0.95952165 -0.34511518 -0.02605191  0.36235172  0.21703477]]\n",
        "200"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  cost 0.140494\n",
        "200  original [ 1.00415492  0.0579232  -0.02486193  0.02608516  0.06309317]\n",
        "200  decoded [[ 0.93643707  0.27358374  0.02356973 -0.09999041  0.23443356]]\n",
        "300  cost 0.0402689\n",
        "300  original [ 1.0126386  -0.17755536 -0.02015486 -0.00586197  0.18370101]\n",
        "300  decoded [[  9.58248794e-01  -2.38349661e-01  -5.09446748e-02   5.04517811e-04\n",
        "    1.62132829e-01]]\n",
        "400"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  cost 0.0570187\n",
        "400  original [ 0.98428588  0.15940471 -0.043853   -0.07726589 -0.02080742]\n",
        "400  decoded [[ 0.9529295   0.19783798 -0.09998219 -0.15587521  0.04601641]]\n",
        "500  cost 0.050589\n",
        "500  original [-0.12483259  0.96481084 -0.17992462  0.10314583 -0.08507154]\n",
        "500  decoded [[-0.1864572   0.95697212 -0.19157848  0.18540731 -0.1301764 ]]\n",
        "600"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  cost 0.0443059\n",
        "600  original [ 0.17886936  1.0398101  -0.00559834  0.12851565 -0.11345431]\n",
        "600  decoded [[ 0.15765414  0.95539254 -0.01078921  0.16751452 -0.13973756]]\n",
        "700  cost 0.0950387\n",
        "700  original [ 1.03392901  0.02162477  0.21715392 -0.16104343 -0.07747009]\n",
        "700  decoded [[ 0.96989626  0.04720913  0.05191991 -0.10499834  0.02234805]]\n",
        "800"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  cost 0.045423\n",
        "800  original [ 0.1066093   1.04642188 -0.04199972 -0.0793786  -0.08932267]\n",
        "800  decoded [[ 0.1636139   0.96277583 -0.04958907 -0.0760427  -0.08817592]]\n",
        "900  cost 0.0857258\n",
        "900  original [ 1.07557418  0.1725498   0.03351972 -0.0020036   0.08919011]\n",
        "900  decoded [[ 0.97334427  0.07531741 -0.03129144  0.06210798  0.18154107]]\n",
        "1000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  cost 0.0518332\n",
        "1000  original [ 0.07071379  1.0010899  -0.02021018  0.09884892 -0.03217925]\n",
        "1000  decoded [[ 0.17063403  0.96260148 -0.02450633  0.13552287 -0.00759195]]\n",
        "1100  cost 0.054529\n",
        "1100  original [ -3.03590645e-04   1.04268472e+00   3.19671330e-01   2.46877986e-02\n",
        "  -5.36822258e-02]\n",
        "1100  decoded [[ 0.05905661  0.96353489  0.38324746  0.01183369 -0.08320495]]\n",
        "1200"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  cost 0.056187\n",
        "1200  original [-0.05666514  1.06756774 -0.05203445  0.04965536  0.21532952]\n",
        "1200  decoded [[ 0.01190439  0.96955669 -0.06323644  0.01604995  0.2302333 ]]\n",
        "1300  cost 0.105865\n",
        "1300  original [-0.04959154  1.0136517   0.02277669 -0.11868522  0.20913257]\n",
        "1300  decoded [[ 0.08455352  0.96480191  0.0937975  -0.19414453  0.36698711]]\n",
        "1400"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  cost 0.0878809\n",
        "1400  original [ 1.036482   -0.05767443  0.13913956 -0.00529949  0.03256074]\n",
        "1400  decoded [[ 0.98089141 -0.22092569  0.15114994  0.08409364  0.00538276]]\n",
        "1500  cost 0.052472\n",
        "1500  original [ 0.90681914  0.06039462  0.04331806  0.30580922 -0.06209438]\n",
        "1500  decoded [[ 0.97159386  0.07399231  0.03433293  0.2121942  -0.03882657]]\n",
        "1600"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  cost 0.130237\n",
        "1600  original [ 0.12336589  0.9936506   0.10225172  0.04270932  0.01706581]\n",
        "1600  decoded [[ 0.37180477  0.94263142  0.21376875  0.1193083   0.06375316]]\n",
        "1700  cost 0.0269036\n",
        "1700  original [-0.16902513  0.99833338  0.10370783 -0.0258051   0.09956441]\n",
        "1700  decoded [[-0.19652823  0.96950328  0.08638176  0.01465747  0.08986882]]\n",
        "1800"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  cost 0.0775382\n",
        "1800  original [ 0.00822399  0.86481933  0.01966114 -0.00126796 -0.05589555]\n",
        "1800  decoded [[ 0.0892253   0.96212345 -0.08456247 -0.03531191 -0.10072898]]\n",
        "1900  cost 0.0806248\n",
        "1900  original [ 0.79654758 -0.0811137  -0.08784621 -0.07755008 -0.05438761]\n",
        "1900  decoded [[ 0.96763283 -0.12616195 -0.05794847 -0.07327698 -0.03735254]]\n",
        "2000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  cost 0.0796197\n",
        "2000  original [ 0.17284114  1.05673218  0.17114875 -0.12903169 -0.07492905]\n",
        "2000  decoded [[  2.60668963e-01   9.61096048e-01   1.64069816e-01  -2.25291252e-01\n",
        "   -6.29294547e-04]]\n",
        "2100  cost 0.0663119\n",
        "2100  original [ 0.84440587 -0.07333846 -0.13713052  0.01198115  0.07897295]\n",
        "2100  decoded [[ 0.973046   -0.11272406 -0.18523006  0.04064457  0.10638919]]\n",
        "2200"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  cost 0.0863363\n",
        "2200  original [-0.10785404  1.10867577  0.06273453  0.05100119  0.10835188]\n",
        "2200  decoded [[-0.03621156  0.96102178  0.05844897  0.14432229  0.14845546]]\n",
        "2300  cost 0.0866198\n",
        "2300  original [ 1.0310728  -0.01724669  0.04019147  0.03118189  0.12003582]\n",
        "2300  decoded [[ 0.97090513 -0.10304361  0.06934606  0.14967154  0.22794257]]\n",
        "2400"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  cost 0.0428316\n",
        "2400  original [ 0.18059967  0.96345902  0.1632423   0.06089789 -0.04381993]\n",
        "2400  decoded [[ 0.21313733  0.95611691  0.11333539 -0.01164495 -0.06134037]]\n",
        "2500  cost 0.0394786\n",
        "2500  original [ 0.92516072  0.01181908  0.01304758 -0.04121358  0.01599252]\n",
        "2500  decoded [[ 0.96608871 -0.05737594  0.02582815 -0.0153791  -0.00634528]]\n",
        "2600"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  cost 0.0653168\n",
        "2600  original [ 1.07572542 -0.04487139  0.0344951   0.03263089 -0.22500797]\n",
        "2600  decoded [[ 0.96477032 -0.1235574   0.02471407  0.03306874 -0.27728638]]\n",
        "2700  cost 0.119257\n",
        "2700  original [ 0.18743632  1.23217927 -0.0752293   0.03233969 -0.01959782]\n",
        "2700  decoded [[ 0.19041826  0.96557248 -0.07540153  0.03659558 -0.02169229]]\n",
        "2800"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  cost 0.125698\n",
        "2800  original [ 1.01169388 -0.04027566 -0.17873084 -0.13434087 -0.044677  ]\n",
        "2800  decoded [[ 0.965186    0.00821953 -0.07094078  0.10864621  0.01715925]]\n",
        "2900  cost 0.0545696\n",
        "2900  original [ 1.04936125  0.03329541 -0.02658098 -0.03558132 -0.07607377]\n",
        "2900  decoded [[ 0.97039223  0.10807911 -0.06573629  0.00347733 -0.07473142]]\n",
        "3000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  cost 0.0543639\n",
        "3000  original [ 1.01923624 -0.10633607 -0.15224908  0.07409215  0.15588786]\n",
        "3000  decoded [[ 0.97171307 -0.21697225 -0.16535394  0.08011658  0.16427298]]\n",
        "3100  cost 0.0778056\n",
        "3100  original [-0.08459705  1.12027382 -0.06105789 -0.04524611 -0.13274481]\n",
        "3100  decoded [[-0.09825682  0.9781549  -0.15597972 -0.02246504 -0.15158845]]\n",
        "3200"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  cost 0.0684121\n",
        "3200  original [ 0.11712566  0.88612836 -0.15943034 -0.10126148 -0.00737504]\n",
        "3200  decoded [[ 0.12768547  0.97125709 -0.22302616 -0.18075517  0.06798428]]\n",
        "3300  cost 0.0738616\n",
        "3300  original [  1.02832400e+00   1.79470923e-02   5.08953739e-04   6.89480419e-02\n",
        "   7.07152604e-03]\n",
        "3300  decoded [[ 0.96840811  0.14172146 -0.01213193  0.15906696  0.01636996]]\n",
        "3400"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  cost 0.086579\n",
        "3400  original [-0.24442473  1.16556411 -0.00838143  0.05896175  0.00778273]\n",
        "3400  decoded [[-0.27573434  0.9769761   0.0150306   0.06805961  0.02518906]]\n",
        "3500  cost 0.080648\n",
        "3500  original [ 1.10247336 -0.03243188 -0.06856704 -0.01966509  0.02378182]\n",
        "3500  decoded [[ 0.97236913 -0.02704902 -0.19279456 -0.01688517  0.0126361 ]]\n",
        "3600"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  cost 0.0260237\n",
        "3600  original [ 1.01055688  0.07908697 -0.16368052 -0.07350522 -0.06007159]\n",
        "3600  decoded [[ 0.97297543  0.08064088 -0.1202039  -0.06561397 -0.05572372]]\n",
        "3700  cost 0.108389\n",
        "3700  original [ 0.07936743  1.20788865 -0.09047527  0.06834443  0.04780851]\n",
        "3700  decoded [[ 0.12097583  0.97016561 -0.10210482  0.08600836  0.05486494]]\n",
        "3800"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  cost 0.0729971\n",
        "3800  original [  9.95186674e-01  -4.21963165e-04  -5.08671296e-02   6.30906298e-02\n",
        "   4.95568954e-02]\n",
        "3800  decoded [[ 0.97489613 -0.10610839  0.04527823  0.12348602  0.00297377]]\n",
        "3900  cost 0.0433959\n",
        "3900  original [ 0.98556681 -0.0428781  -0.0055408   0.01450447  0.17252359]\n",
        "3900  decoded [[ 0.97477347 -0.1384698   0.00406217  0.0175858   0.18027246]]\n",
        "4000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  cost 0.0580612\n",
        "4000  original [  1.07013786e+00   6.65780089e-02  -8.85132473e-02   1.20823968e-01\n",
        "   2.70051761e-04]\n",
        "4000  decoded [[ 0.9711712   0.0032811  -0.05877378  0.08408787  0.02891689]]\n",
        "4100  cost 0.0603148\n",
        "4100  original [ 1.02161065 -0.00366448 -0.09884801 -0.0683644  -0.06305679]\n",
        "4100  decoded [[ 0.97311562  0.08357333 -0.18131937 -0.10342594 -0.04904537]]\n",
        "4200"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  cost 0.0665091\n",
        "4200  original [-0.12460951  0.86248747  0.04223689 -0.07642008  0.01195449]\n",
        "4200  decoded [[-0.21313733  0.96402794  0.07401618 -0.12978114  0.02255207]]\n",
        "4300  cost 0.0360237\n",
        "4300  original [ 0.0308707   0.9820963  -0.04862588  0.02463763  0.1604559 ]\n",
        "4300  decoded [[ 0.04061347  0.95962769 -0.05184819  0.00955229  0.08528387]]\n",
        "4400"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  cost 0.0697195\n",
        "4400  original [ 0.00565845  1.11462573  0.08855526 -0.19730446 -0.11310592]\n",
        "4400  decoded [[ 0.04095313  0.96284443  0.09284584 -0.19753551 -0.11159153]]\n",
        "4500  cost 0.113788\n",
        "4500  original [ 0.72959469 -0.1249113   0.05998461  0.04611127  0.07364183]\n",
        "4500  decoded [[ 0.97064996 -0.20583029  0.0690607   0.04672116  0.07392722]]\n",
        "4600"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  cost 0.0535123\n",
        "4600  original [ 0.25261103  0.93165411  0.18821714  0.06656974  0.02077268]\n",
        "4600  decoded [[ 0.32265145  0.94893736  0.16717008 -0.00150404 -0.04276036]]\n",
        "4700  cost 0.0683163\n",
        "4700  original [-0.10476079  1.11985227 -0.0815741  -0.04341328 -0.05936036]\n",
        "4700  decoded [[-0.08657672  0.96922809 -0.09476337 -0.05488581 -0.05594175]]\n",
        "4800"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  cost 0.0704375\n",
        "4800  original [-0.10106094  0.89311214  0.04567969 -0.25331015 -0.04698731]\n",
        "4800  decoded [[ 0.00830417  0.96420038  0.00195676 -0.20002741 -0.10214318]]\n",
        "4900  cost 0.0504442\n",
        "4900  original [-0.0567702   1.0298319   0.09703247  0.04576123  0.04080599]\n",
        "4900  decoded [[-0.14653027  0.96692544  0.09009106  0.02006134  0.04024404]]\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}